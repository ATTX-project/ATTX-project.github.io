<h1 style="color:red">DRAFT - work in progress</h1>

# ISWC 2017 Demo and Poster

https://iswc2017.semanticweb.org/calls/call-for-posters-and-demos/ Deadline July 27.

## Proposal 1

Title: Practical Data Provenance for Linked Data Brokers in a Microservices Architecture

### Abstract


### Paper Contents

Maintaining some sort of data provenance, i.e. the data about the actions that have led the target data to its current state, is an integral feature of a system acting as a data broker. After all, brokering can be seen as an activity that transforms external data sources, which one might not have any control over, into new data source. This transformation can involve complex processing steps, which all contribute to the provenance data. Keeping track of who did what, why and when, is therefore necessary in order be able to ascribe responsibility of, e.g. data quality, to the right (human or software) entity.

We present the result of the ATTX project, which provides a set of software components that can be used to build scalable data brokers that work on linked data. We will cover issues and implementation related to modelling, acquisition, exposing and using provenance information produced by services that comprise the ATTX data broker instance.

The ATTX Workflow ontology has the purpose of describing how and when the data flows within the ATTX platform, types of activities or steps, associated workflows and workflow executions, the provenance information (agent and ETL processes performed) and other metadata.

Its main goal is to abstract the type of data utilised in the workflow and instead focus on the metadata that can be disseminated along with the workflow such as the steps (including the step types and configuration), the input and output datasets along with the activities performed to achieve the end result.


Functionalities we identified across the use cases and we want them to be part of the services provided by the broker.
A Semantic Broker is not just passing data around, the data is there, it collects the data, that it later uses in the services it provides, so it is more that just simple transformation or simple services it keeps track of all the transformations and processes related to data.
When acting as a broker you know where the data provide for reuse came from and where the mistakes or missing data and we have to know the current state and why.

## ATTX Ontology: An extension of PROV-O

ATTX Workflow Ontology builds upon existing ontologies such as PROV-O and PWO, however due to its specific purpose it cannot make use of all the concepts and properties particular to these ontologies.

The ontology aims to provide a baseline required to capture and analyze data acquisition, enrichment and dissemination workflows. Thus, it covers the main aspects that correspond to prospective provenance [ZWF06] additionally, some essential elements of data services are also considered. A prospective provenance ontology "captures the procedure or "recipe" required to produce a data artifact." [ProvONE]

Extended and Similar Ontologies

The Publishing Workflow Ontology (PWO) has as a main goal describing the main stages in the workflow associated with the publication of a document.

Another related Workflow ontology is the OPMW-PROV Ontology that builds also on top of PROV-O, and at the same time The P-plan Ontology and The Open Provenance Model Ontology (OPMO). It focus is on workflow traces and their templates based, and similar to PWO, with a focus on scientific articles and their results.

ProvONE provides a more holistic overview on the Workflow and provenance infomation, yet still with a focus on Scientific Workflow Provenance.

Current model considers current ETL tools in processing and generating Semantic Web data such as:

connection with the logging ontology


## Proposal 2

Automatic Deployment of a Linked Data Broker in Distributed Environment

### Abstract
Our  a Linked Data Broker solution based on Semantic Web Technologies that is both flexible and extendible in terms of incoming and outgoing data, as well as the cloud based infrastructural resources employed to operate the such a platform. Our solution consists of components implementing different types of services such as workflow and graph management, processing, distribution and provenance.

### Paper Contents

Advantages, integrated and uniform testing in all environments PC, Cloud, CI/CD testing environment

We identify three levels of automation:
* CI (Continuous Integration)/CD (Continuous Development) Automation
* Deployment Automation
* Data Distribution Automation

![Figure 1. ATTX Platform Architecture](images/platform_architecture.png)

ATTX Semantic Broker is composed of a collection of loosely coupled services, which implement several broker-like capabilities. Example of such services are illustrated in: **[Microservices Architecture](#microservices-example)**.

Given that there are a collection of such services, we can still identify three core components under which we can cluster the services: **Workflow Component**, **Graph Component**, **Distribution Component** while the **Service Discovery** and the **Semantic Broker Deployment** gather all of them together in order to provide them as a individual services or as a platform. The communication and contracts is done after a server-side services discovery pattern[reference] as it ...

* **Workflow Component** provides a configurable framework with the main purpose of managing, scheduling and monitoring data ingestion, processing and distribution related workflows. Workflow Component provides the Semantic Broker information related to provenance of the working data.

* **Graph Component** associated to this project has the main goal of aggregating the data that flows within the Semantic Broker, types of transformations (and associated workflows), the provenance information (agent and ETL processes performed) and other meta data.

* **Distribution Component** provides the interface between the Workflow Component and/or Graph Component for public consumption of disseminated data.


Having different components in containers; a problem with prebuilt software

The aim of providing easily deployable components for building custom linked data services on top of infrastructure-as-a-service platform	.
We want all of these to run in containers and to follow a microservices oriented architecture and we want to achieve this as quick and easiest as possible.

Now how much automation is too much ?
Should/can we automate everything ?



References:
* [Applying the Virtual Data Provenance Model](https://link.springer.com/chapter/10.1007/11890850_16)
* [The Publishing Workflow Ontology (PWO)](http://www.semantic-web-journal.net/content/publishing-workflow-ontology-pwo)
* [Automatic capture and reconstruction of computational provenance](http://onlinelibrary.wiley.com/doi/10.1002/cpe.1247/full )
