<h1 style="color:red">DRAFT - work in progress</h1>

# ISWC 2017 Demo and Poster

https://iswc2017.semanticweb.org/calls/call-for-posters-and-demos/ Deadline July 27.

## Proposal 1

Title: Practical Data Provenance for Linked Data Brokers in a Microservices Architecture

### Abstract

The openness of the Web and the ease to combine linked
data from different sources creates new challenges. Systems
that consume linked data must evaluate quality and trustworthiness
of the data. A common approach for data quality
assessment is the analysis of provenance information. For
this reason, this paper discusses provenance of data on the
Web and proposes a suitable provenance model. While traditional
provenance research usually addresses the creation
of data, our provenance model also represents data access,
a dimension of provenance that is particularly relevant in
the context of Web data. Based on our model we identify
options to obtain provenance information and we raise open
questions concerning the publication of provenance-related
metadata for linked data on the Web.



### Paper Contents

#### Introduction

Maintaining some sort of data provenance, inherently information about the actions that have led a data set to its current state, is an integral feature of any system acting as a data broker. In esence a data brokering system can be seen as a system that has provides services that transforms external data sources, which one system might have direct control or not, into new data source. The series of transformations can involve complex, and long lasting processing steps, however all of them contribute to the provenance data. The broker's job is keeping track of what component and part of the system did what, why and for how long, therefore it is necessary to be able to provide traceability and via certain mesasures, e.g. data quality, ascrive responsibility to the right (human or software) entity. When acting as a broker you know where the data provide for reuse came from and where the mistakes or missing data and we have to know the current state and why.

While this might sound like an ETL (Extract Transform Load) tool, a Semantic Broker is not just passing data around, it is more that just simple transformation or simple services, it keeps track of all the transformations and processes related to data, it adds value with every transformation by generating new data, or providing access to the generated data sets.

The ATTX Workflow ontology has the purpose of describing how and when the data flows within the ATTX platform, types of activities or steps, associated workflows and workflow executions, the provenance information (agent and ETL processes performed) and other metadata.

Its main goal is to abstract the type of data utilised in the workflow and instead focus on the metadata that can be disseminated along with the workflow such as the steps (including the step types and configuration), the input and output datasets along with the activities performed to achieve the end result.

Functionalities we identified across the use cases and we want them to be part of the services provided by the broker.

#### ATTX Ontology: An extension of PROV-O

ATTX Workflow Ontology builds upon existing ontologies such as PROV-O and PWO, however due to its specific purpose it cannot make use of all the concepts and properties particular to these ontologies.

The ontology aims to provide a baseline required to capture and analyze data acquisition, enrichment and dissemination workflows. Thus, it covers the main aspects that correspond to prospective provenance [ZWF06] additionally, some essential elements of data services are also considered. A prospective provenance ontology "captures the procedure or "recipe" required to produce a data artifact." [ProvONE]

Extended and Similar Ontologies

The Publishing Workflow Ontology (PWO) has as a main goal describing the main stages in the workflow associated with the publication of a document.

Another related Workflow ontology is the OPMW-PROV Ontology that builds also on top of PROV-O, and at the same time The P-plan Ontology and The Open Provenance Model Ontology (OPMO). It focus is on workflow traces and their templates based, and similar to PWO, with a focus on scientific articles and their results.

ProvONE provides a more holistic overview on the Workflow and provenance infomation, yet still with a focus on Scientific Workflow Provenance.

Current model considers current ETL tools in processing and generating Semantic Web data such as:

connection with the logging ontology

#### Provenance Service


#### Conclusions and Future Work


References:
* [Applying the Virtual Data Provenance Model](https://link.springer.com/chapter/10.1007/11890850_16)
* [The Publishing Workflow Ontology (PWO)](http://www.semantic-web-journal.net/content/publishing-workflow-ontology-pwo)
* [Automatic capture and reconstruction of computational provenance](http://onlinelibrary.wiley.com/doi/10.1002/cpe.1247/full )


## Proposal 2

Automatic Deployment of a Linked Data Broker in a Distributed Environment

### Abstract
Handling multiple environments as well as automatic deployment and scaling of infrastructure is becoming a must in the development of any data crunching platform. This paper illustrates a Linked Data Broker solution based on Semantic Web Technologies that is both flexible and extendible in terms of incoming and outgoing data, as well as the cloud based infrastructural resources employed to operate the such a platform. Our solution consists of components implementing different types of services such as workflow and graph management, processing, distribution and provenance. The focus of the solution is on: easy development, easy deployment and simple publication of redistributed data with openly available and usable provenance data.

### Paper Contents

#### Introduction

We present the result of the ATTX project, which provides a set of software components that can be used to build scalable data brokers that work on linked data. We will cover issues and implementation related to modelling, acquisition, exposing and using provenance information produced by services that comprise the ATTX data broker instance.

architecture of our proposed semantic data broker solution for Linked Data services, how it differs from existing solutions, and how it can be (re)used in an automated way by other projects and organisations, not exclusively in a library environment.

Advantages, integrated and uniform testing in all environments PC, Cloud, CI/CD testing environment

We identify three levels of automation:
* CI (Continuous Integration)/CD (Continuous Development) Automation
* Deployment Automation
* Data Distribution Automation

#### Linked Data Brocker Overview

![Figure 1. ATTX Platform Architecture](images/platform_architecture.png)

ATTX Semantic Broker is composed of a collection of loosely coupled services, which implement several broker-like capabilities. Example of such services are illustrated in: **[Microservices Architecture](#microservices-example)**.

Given that there are a collection of such services, we can still identify three core components under which we can cluster the services: **Workflow Component**, **Graph Component**, **Distribution Component** while the **Service Discovery** and the **Semantic Broker Deployment** gather all of them together in order to provide them as a individual services or as a platform. The communication and contracts is done after a server-side services discovery pattern[reference] as it ...

* **Workflow Component** provides a configurable framework with the main purpose of managing, scheduling and monitoring data ingestion, processing and distribution related workflows. Workflow Component provides the Semantic Broker information related to provenance of the working data.

* **Graph Component** associated to this project has the main goal of aggregating the data that flows within the Semantic Broker, types of transformations (and associated workflows), the provenance information (agent and ETL processes performed) and other meta data.

* **Distribution Component** provides the interface between the Workflow Component and/or Graph Component for public consumption of disseminated data.


Having different components in containers; a problem with prebuilt software

The aim of providing easily deployable components for building custom linked data services on top of infrastructure-as-a-service platform	.
We want all of these to run in containers and to follow a microservices oriented architecture and we want to achieve this as quick and easiest as possible.

Now how much automation is too much ?
Should/can we automate everything ?

#### Automatic Deployment


#### Conclusions and Future Work


References:
* [The BigDataEurope Platform â€“ Supporting the Variety Dimension of Big Data](https://link.springer.com/chapter/10.1007/978-3-319-60131-1_3)
